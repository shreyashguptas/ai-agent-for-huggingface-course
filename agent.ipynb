{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install langfuse wikipedia openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config loaded. Sensitive keys available for use.\n",
      "Notebook variables set:\n",
      "  model_name: gpt-4.1-2025-04-14\n",
      "  temperature: 0.0\n",
      "  verbose: True\n",
      "  use_langfuse: True\n",
      "  username: Shreyashgupta5\n",
      "  code_link: https://huggingface.co/spaces/Shreyashgupta5/ai_agents_course\n",
      "  api_base_url: https://agents-course-unit4-scoring.hf.space\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load Configuration and Dependencies\n",
    "\n",
    "import json\n",
    "\n",
    "# Load sensitive config from config.json\n",
    "with open('config.json', 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# Set notebook variables (these should match what you set in your notebook)\n",
    "temperature = 0.0\n",
    "verbose = True\n",
    "use_langfuse = True\n",
    "model_name = \"gpt-4.1-2025-04-14\"\n",
    "username = \"Shreyashgupta5\"\n",
    "code_link = \"https://huggingface.co/spaces/Shreyashgupta5/ai_agents_course\"\n",
    "api_base_url = \"https://agents-course-unit4-scoring.hf.space\"\n",
    "\n",
    "# Print to verify\n",
    "print(\"Config loaded. Sensitive keys available for use.\")\n",
    "print(\"Notebook variables set:\")\n",
    "print(f\"  model_name: {model_name}\")\n",
    "print(f\"  temperature: {temperature}\")\n",
    "print(f\"  verbose: {verbose}\")\n",
    "print(f\"  use_langfuse: {use_langfuse}\")\n",
    "print(f\"  username: {username}\")\n",
    "print(f\"  code_link: {code_link}\")\n",
    "print(f\"  api_base_url: {api_base_url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task ID: 8e867cd7-cff9-4e6c-867a-ff5ddc2550be\n",
      "Question: How many studio albums were published by Mercedes Sosa between 2000 and 2009 (included)? You can use the latest 2022 version of english wikipedia.\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Load Questions\n",
    "\n",
    "import json\n",
    "\n",
    "# Load all questions from 1_question.json\n",
    "with open('1_question.json', 'r') as f:\n",
    "    questions = json.load(f)\n",
    "\n",
    "# Print out each question's task_id and question text for verification\n",
    "for q in questions:\n",
    "    print(f\"Task ID: {q['task_id']}\")\n",
    "    print(f\"Question: {q['question']}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Langfuse tracing initialized.\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Initialize Langfuse Tracing\n",
    "\n",
    "from langfuse import Langfuse\n",
    "\n",
    "# Initialize Langfuse with credentials from config\n",
    "langfuse = Langfuse(\n",
    "    public_key=config[\"langfuse_public_key\"],\n",
    "    secret_key=config[\"langfuse_secret\"],\n",
    "    host=config[\"host\"]\n",
    ")\n",
    "\n",
    "print(\"Langfuse tracing initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Define Tools\n",
    "\n",
    "import wikipedia\n",
    "from langfuse.decorators import observe  # (if not already imported)\n",
    "\n",
    "@observe()\n",
    "def wikipedia_search(query, sentences=2):\n",
    "    \"\"\"\n",
    "    Search Wikipedia for a query and return a summary.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        summary = wikipedia.summary(query, sentences=sentences, auto_suggest=True, redirect=True)\n",
    "        return summary\n",
    "    except wikipedia.DisambiguationError as e:\n",
    "        return f\"Disambiguation error. Options: {e.options[:5]}\"\n",
    "    except wikipedia.PageError:\n",
    "        return \"No Wikipedia page found for the query.\"\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example usage:\n",
    "# result = wikipedia_search(\"Mercedes Sosa\")\n",
    "# print(\"Wikipedia Search Result:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent's Plan:\n",
      " Step-by-Step Plan to Answer the Question:\n",
      "\n",
      "1. **Identify the Subject**  \n",
      "   - Subject: Mercedes Sosa, an Argentine singer.\n",
      "\n",
      "2. **Clarify the Time Frame**  \n",
      "   - Years of interest: 2000 to 2009 (inclusive).\n",
      "\n",
      "3. **Select the Main Source**  \n",
      "   - Tool: The latest (2022) version of the English Wikipedia article on Mercedes Sosa.\n",
      "\n",
      "4. **Locate the Discography Section**  \n",
      "   - Navigate to the \"Discography\" section of the Wikipedia article.\n",
      "   - Look specifically for a list of \"Studio albums\" (as opposed to live albums, compilations, etc.).\n",
      "\n",
      "5. **Extract Relevant Data**  \n",
      "   - List all studio albums with their release years.\n",
      "   - Identify which albums were released between 2000 and 2009 (inclusive).\n",
      "\n",
      "6. **Count the Albums**  \n",
      "   - Count the number of studio albums published in the specified time frame.\n",
      "\n",
      "7. **Double-Check for Accuracy**  \n",
      "   - Ensure that only studio albums are counted (exclude live, compilation, or collaborative albums unless they are explicitly listed as studio albums).\n",
      "\n",
      "**Summary of Tools and Usage:**\n",
      "- **Wikipedia (2022 English version):** Main source for discography and album release years.\n",
      "- **Manual Filtering:** To ensure only studio albums within the specified years are counted.\n",
      "\n",
      "**Optional Tools:**\n",
      "- **Wikipedia Page Search (Ctrl+F):** To quickly find \"Discography\" and \"Studio albums\" sections.\n",
      "- **Cross-reference with Other Reliable Sources:** (If needed) such as AllMusic or official artist website, to confirm the accuracy of Wikipedia data.\n",
      "\n",
      "**End Result:**  \n",
      "A precise count of Mercedes Sosa's studio albums released between 2000 and 2009, based on the 2022 English Wikipedia.\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Agent Planning Step (OpenAI v1.x+)\n",
    "\n",
    "import openai\n",
    "from langfuse.decorators import observe  # (if not already imported)\n",
    "\n",
    "# Set your OpenAI API key from config\n",
    "client = openai.OpenAI(api_key=config[\"openai_api_key\"])\n",
    "\n",
    "@observe(as_type=\"generation\")\n",
    "def get_agent_plan(question, model_name, temperature=0.0):\n",
    "    \"\"\"\n",
    "    Sends the question to the model and asks for a plan and tool list.\n",
    "    \"\"\"\n",
    "    prompt = (\n",
    "        \"You are an AI agent. Here is a question you need to answer:\\n\"\n",
    "        f\"Question: {question}\\n\\n\"\n",
    "        \"Create a step-by-step plan to answer this question. \"\n",
    "        \"List the tools you would use and explain briefly how you would use them.\"\n",
    "    )\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=temperature\n",
    "    )\n",
    "    plan = response.choices[0].message.content\n",
    "    return plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wikipedia tool executed.\n",
      "Tool Outputs: {'wikipedia': 'Shakira Isabel Mebarak Ripoll ( shə-KEER-ə, Spanish: [ʃaˈkiɾa isaˈβel meβaˈɾak riˈpol]; born 2 February 1977), known mononymously as Shakira, is a Colombian singer-songwriter. She has had a significant impact on the musical landscape of Latin America and has been credited with popularizing Hispanophone music on a global level.'}\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Tool Execution Step\n",
    "\n",
    "from langfuse.decorators import observe  # (if not already imported)\n",
    "\n",
    "@observe()\n",
    "def execute_tools(plan, question):\n",
    "    \"\"\"\n",
    "    Executes tools as suggested in the plan.\n",
    "    For now, only supports Wikipedia search.\n",
    "    Returns a dictionary of tool outputs.\n",
    "    \"\"\"\n",
    "    tool_outputs = {}\n",
    "    if \"wikipedia\" in plan.lower():\n",
    "        wiki_result = wikipedia_search(question)\n",
    "        tool_outputs['wikipedia'] = wiki_result\n",
    "        print(\"Wikipedia tool executed.\")\n",
    "    else:\n",
    "        print(\"No supported tools found in the plan.\")\n",
    "    return tool_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Answer JSON:\n",
      " ```json\n",
      "{\n",
      "  \"task_id\": \"gaia_2024_1042\",\n",
      "  \"submitted_answer\": \"3\",\n",
      "  \"reasoning_trace\": \"Mercedes Sosa released three studio albums between 2000 and 2009: 'Misa Criolla' (2000), 'Acústico' (2002), and 'Cantora 1' (2009), according to the 2022 English Wikipedia discography.\"\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Synthesis Step (OpenAI v1.x+)\n",
    "\n",
    "from langfuse.decorators import observe  # (if not already imported)\n",
    "\n",
    "@observe(as_type=\"generation\")\n",
    "def synthesize_final_answer(task_id, question, tool_outputs, gaia_doc, model_name, temperature=0.0):\n",
    "    \"\"\"\n",
    "    Uses the model to synthesize a final answer in GAIA format.\n",
    "    \"\"\"\n",
    "    prompt = (\n",
    "        f\"You are an AI agent participating in the GAIA benchmark. \"\n",
    "        f\"Here is the official GAIA documentation for answer formatting:\\n\\n\"\n",
    "        f\"{gaia_doc}\\n\\n\"\n",
    "        f\"Here is the original question:\\n{question}\\n\\n\"\n",
    "        f\"Here are the outputs from the tools you used:\\n{tool_outputs}\\n\\n\"\n",
    "        \"Using the information above, generate the final answer in the required GAIA JSON format. \"\n",
    "        \"Only output the JSON object, nothing else.\"\n",
    "    )\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=temperature\n",
    "    )\n",
    "    final_answer_json = response.choices[0].message.content\n",
    "    return final_answer_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7.5: Main Agent Loop with Langfuse Traceability\n",
    "\n",
    "from langfuse.decorators import observe, langfuse_context  # (if not already imported)\n",
    "\n",
    "@observe()\n",
    "def process_all_questions(questions, model_name, temperature, gaia_doc):\n",
    "    final_answers = []\n",
    "    for q in questions:\n",
    "        print(f\"Processing Task ID: {q['task_id']}\")\n",
    "        plan = get_agent_plan(q['question'], model_name, temperature)\n",
    "        tool_outputs = execute_tools(plan, q['question'])\n",
    "        # --- FIX: Use the task_id from the question directly ---\n",
    "        final_answer_json = synthesize_final_answer(\n",
    "            task_id=q['task_id'],  # <-- This ensures the output matches the input question\n",
    "            question=q['question'],\n",
    "            tool_outputs=tool_outputs,\n",
    "            gaia_doc=gaia_doc,\n",
    "            model_name=model_name,\n",
    "            temperature=temperature\n",
    "        )\n",
    "        final_answers.append(final_answer_json)\n",
    "        # Print trace URL for traceability\n",
    "        print(\"Langfuse Trace URL:\", langfuse_context.get_current_trace_url())\n",
    "    return final_answers\n",
    "\n",
    "# Load GAIA documentation from file\n",
    "with open(\"documentation/GIAI-documentation.md\", \"r\") as f:\n",
    "    gaia_doc = f.read()\n",
    "\n",
    "# Example usage:\n",
    "final_answers = process_all_questions(questions, model_name, temperature, gaia_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 1 answers to final_answers.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Step 8: Save Results (with cleaning and validation)\n",
    "\n",
    "import json\n",
    "import re\n",
    "\n",
    "def clean_and_validate_answer(answer_str, required_fields=(\"task_id\", \"submitted_answer\")):\n",
    "    \"\"\"\n",
    "    Cleans markdown code block from model output and validates required fields.\n",
    "    Returns a dict if valid, else raises ValueError.\n",
    "    \"\"\"\n",
    "    # Remove markdown code block if present\n",
    "    answer_str = answer_str.strip()\n",
    "    if answer_str.startswith(\"```\"):\n",
    "        # Remove the opening ```json or ```\n",
    "        answer_str = re.sub(r\"^```[a-zA-Z]*\", \"\", answer_str)\n",
    "        # Remove the closing ```\n",
    "        answer_str = re.sub(r\"```$\", \"\", answer_str).strip()\n",
    "    # Parse JSON\n",
    "    try:\n",
    "        answer_obj = json.loads(answer_str)\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Invalid JSON: {e}\\nRaw output: {answer_str}\")\n",
    "    # Check required fields\n",
    "    for field in required_fields:\n",
    "        if field not in answer_obj:\n",
    "            raise ValueError(f\"Missing required field '{field}' in answer: {answer_obj}\")\n",
    "    return answer_obj\n",
    "\n",
    "def save_final_answers(final_answers, filename=\"final_answers.jsonl\"):\n",
    "    \"\"\"\n",
    "    Saves a list of final answer dicts or JSON strings to a .jsonl file.\n",
    "    Each answer should be a valid JSON object (dict or JSON string).\n",
    "    Cleans and validates each answer before saving.\n",
    "    \"\"\"\n",
    "    cleaned_answers = []\n",
    "    for i, answer in enumerate(final_answers):\n",
    "        # If answer is a string, clean and validate\n",
    "        if isinstance(answer, str):\n",
    "            try:\n",
    "                answer_obj = clean_and_validate_answer(answer)\n",
    "            except Exception as e:\n",
    "                print(f\"Error in answer {i}: {e}\")\n",
    "                continue\n",
    "        else:\n",
    "            answer_obj = answer\n",
    "        cleaned_answers.append(answer_obj)\n",
    "    # Write to file\n",
    "    with open(filename, \"w\") as f:\n",
    "        for answer_obj in cleaned_answers:\n",
    "            f.write(json.dumps(answer_obj, ensure_ascii=False) + \"\\n\")\n",
    "    print(f\"Saved {len(cleaned_answers)} answers to {filename}\")\n",
    "\n",
    "# Example usage:\n",
    "# Suppose you have a list of final answer JSON strings or dicts:\n",
    "# final_answers = [final_answer_json, ...]  # Add more as you loop through all questions\n",
    "\n",
    "save_final_answers(final_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission failed: 422 {\"detail\":[{\"type\":\"missing\",\"loc\":[\"body\",\"agent_code\"],\"msg\":\"Field required\",\"input\":{\"username\":\"Shreyashgupta5\",\"code_link\":\"https://huggingface.co/spaces/Shreyashgupta5/ai_agents_course\",\"answers\":[\"```json\\n{\\n  \\\"task_id\\\": \\\"gaia_2024_1042\\\",\\n  \\\"submitted_answer\\\": \\\"3\\\",\\n  \\\"reasoning_trace\\\": \\\"Mercedes Sosa released three studio albums between 2000 and 2009: 'Misa Criolla' (2000), 'Acústico' (2002), and 'Cantora 1' (2009), according to the 2022 English Wikipedia discography.\\\"\\n}\\n```\"]}},{\"type\":\"model_attributes_type\",\"loc\":[\"body\",\"answers\",0],\"msg\":\"Input should be a valid dictionary or object to extract fields from\",\"input\":\"```json\\n{\\n  \\\"task_id\\\": \\\"gaia_2024_1042\\\",\\n  \\\"submitted_answer\\\": \\\"3\\\",\\n  \\\"reasoning_trace\\\": \\\"Mercedes Sosa released three studio albums between 2000 and 2009: 'Misa Criolla' (2000), 'Acústico' (2002), and 'Cantora 1' (2009), according to the 2022 English Wikipedia discography.\\\"\\n}\\n```\"}]}\n"
     ]
    }
   ],
   "source": [
    "# Step 9: Validate Answers (Check with GAIA API) -- FIXED\n",
    "\n",
    "import requests\n",
    "\n",
    "def validate_answers(final_answers, username, code_link, api_base_url, agent_code=None):\n",
    "    \"\"\"\n",
    "    Submits answers to the GAIA evaluation endpoint for validation.\n",
    "    Prints the score and which answers were correct.\n",
    "    \"\"\"\n",
    "    url = f\"{api_base_url}/submit\"\n",
    "    if agent_code is None:\n",
    "        # Try to read your notebook as code, or use code_link as fallback\n",
    "        try:\n",
    "            with open(\"agent.ipynb\", \"r\") as f:\n",
    "                agent_code = f.read()\n",
    "        except Exception:\n",
    "            agent_code = code_link  # fallback\n",
    "    payload = {\n",
    "        \"username\": username,\n",
    "        \"code_link\": code_link,\n",
    "        \"agent_code\": agent_code,\n",
    "        \"answers\": final_answers\n",
    "    }\n",
    "    response = requests.post(url, json=payload)\n",
    "    if response.status_code == 200:\n",
    "        result = response.json()\n",
    "        print(\"Submission successful!\")\n",
    "        print(f\"Score: {result.get('score', 'N/A')}%\")\n",
    "        if \"results\" in result:\n",
    "            print(\"\\nDetailed Results:\")\n",
    "            for r in result[\"results\"]:\n",
    "                status = \"✅\" if r.get(\"correct\") else \"❌\"\n",
    "                print(f\"{status} Task ID: {r['task_id']} | Your Answer: {r['submitted_answer']} | Correct: {r.get('correct_answer', 'N/A')}\")\n",
    "        else:\n",
    "            print(\"No detailed results returned.\")\n",
    "        return result\n",
    "    else:\n",
    "        print(\"Submission failed:\", response.status_code, response.text)\n",
    "        return None\n",
    "\n",
    "# Example usage:\n",
    "validation_result = validate_answers(final_answers, username, code_link, api_base_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 10: (Optional) Save Validation Results\n",
    "\n",
    "def save_validation_results(validation_result, filename=\"validation_results.json\"):\n",
    "    if validation_result is not None:\n",
    "        with open(filename, \"w\") as f:\n",
    "            json.dump(validation_result, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"Validation results saved to {filename}\")\n",
    "\n",
    "# Example usage:\n",
    "save_validation_results(validation_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 11: Save Validation Results (with cleaning and validation)\n",
    "\n",
    "# --- Langfuse flush at the end of the notebook ---\n",
    "from langfuse.decorators import langfuse_context  # (if not already imported)\n",
    "langfuse_context.flush()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_agents_course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
