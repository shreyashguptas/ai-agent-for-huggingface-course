{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -q langfuse wikipedia openai google-search-results pandas openai-whisper ffmpeg-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load Configuration and Dependencies\n",
    "\n",
    "import json\n",
    "\n",
    "# Load sensitive config from config.json\n",
    "with open('config.json', 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# Set notebook variables (these should match what you set in your notebook)\n",
    "temperature = 0.2\n",
    "verbose = True\n",
    "use_langfuse = True\n",
    "# model_name = \"o4-mini-2025-04-16\"\n",
    "model_name = \"gpt-4.1-2025-04-14\"\n",
    "username = \"Shreyashgupta5\"\n",
    "code_link = \"https://huggingface.co/spaces/Shreyashgupta5/ai_agents_course\"\n",
    "api_base_url = \"https://agents-course-unit4-scoring.hf.space\"\n",
    "\n",
    "# Print to verify\n",
    "print(\"Config loaded. Sensitive keys available for use.\")\n",
    "print(\"Notebook variables set:\")\n",
    "print(f\"  model_name: {model_name}\")\n",
    "print(f\"  temperature: {temperature}\")\n",
    "print(f\"  verbose: {verbose}\")\n",
    "print(f\"  use_langfuse: {use_langfuse}\")\n",
    "print(f\"  username: {username}\")\n",
    "print(f\"  code_link: {code_link}\")\n",
    "print(f\"  api_base_url: {api_base_url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Configure Langfuse Decorator-Based Client\n",
    "\n",
    "from langfuse.decorators import langfuse_context\n",
    "\n",
    "langfuse_context.configure(\n",
    "    secret_key=config[\"langfuse_secret\"],\n",
    "    public_key=config[\"langfuse_public_key\"],\n",
    "    host=config[\"host\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Load Questions and Download Required Files\n",
    "\n",
    "import json\n",
    "import os\n",
    "import requests\n",
    "\n",
    "# Set how many questions you want to process\n",
    "NUM_QUESTIONS_TO_RUN = 20  # <--- Change this number as needed\n",
    "\n",
    "# Load all questions from all-json/all_questions.json\n",
    "with open('all-json/all_questions.json', 'r') as f:\n",
    "    all_questions = json.load(f)\n",
    "\n",
    "# Only process up to NUM_QUESTIONS_TO_RUN questions\n",
    "questions = all_questions[:NUM_QUESTIONS_TO_RUN]\n",
    "\n",
    "# Helper: Download file if it's not already present (supports images, Excel, CSV, etc.)\n",
    "def download_file_if_needed(q, api_base_url):\n",
    "    file_name = q.get(\"file_name\")\n",
    "    if file_name:\n",
    "        file_url = f\"{api_base_url}/files/{q['task_id']}\"\n",
    "        file_path = os.path.join(\"files-for-agent\", file_name)\n",
    "        if not os.path.exists(file_path):\n",
    "            r = requests.get(file_url)\n",
    "            if r.status_code == 200:\n",
    "                with open(file_path, \"wb\") as f:\n",
    "                    f.write(r.content)\n",
    "                print(f\"Downloaded file for task {q['task_id']}: {file_path}\")\n",
    "            else:\n",
    "                print(f\"Failed to download file for task {q['task_id']}: {r.status_code}\")\n",
    "\n",
    "# Download files for relevant questions (images, Excel, CSV, etc.)\n",
    "for q in questions:\n",
    "    download_file_if_needed(q, api_base_url)\n",
    "\n",
    "# Print out each question's task_id and question text for verification\n",
    "for q in questions:\n",
    "    print(f\"Task ID: {q['task_id']}\")\n",
    "    print(f\"Question: {q['question']}\")\n",
    "    if q.get(\"file_name\"):\n",
    "        print(f\"File: files-for-agent/{q['file_name']}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Define Tools\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import wikipedia\n",
    "import pandas as pd\n",
    "from langfuse.decorators import observe\n",
    "import base64\n",
    "import mimetypes\n",
    "\n",
    "# Wikipedia Search Tool\n",
    "@observe()\n",
    "def wikipedia_search(query, sentences=2):\n",
    "    try:\n",
    "        summary = wikipedia.summary(query, sentences=sentences, auto_suggest=True, redirect=True)\n",
    "        return summary\n",
    "    except wikipedia.DisambiguationError as e:\n",
    "        return f\"Disambiguation error. Options: {e.options[:5]}\"\n",
    "    except wikipedia.PageError:\n",
    "        return \"No Wikipedia page found for the query.\"\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "# SerpAPI Web Search Tool\n",
    "@observe()\n",
    "def serpapi_search(query):\n",
    "    api_key = config.get(\"SERPAPI_API_KEY\") or os.environ.get(\"SERPAPI_API_KEY\")\n",
    "    if not api_key:\n",
    "        return \"No SerpAPI key provided.\"\n",
    "    params = {\n",
    "        \"q\": query,\n",
    "        \"api_key\": api_key,\n",
    "        \"engine\": \"google\",\n",
    "        \"num\": 3\n",
    "    }\n",
    "    response = requests.get(\"https://serpapi.com/search\", params=params)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        if \"answer_box\" in data and \"answer\" in data[\"answer_box\"]:\n",
    "            return data[\"answer_box\"][\"answer\"]\n",
    "        elif \"organic_results\" in data and len(data[\"organic_results\"]) > 0:\n",
    "            return data[\"organic_results\"][0].get(\"snippet\", \"No snippet found.\")\n",
    "        else:\n",
    "            return \"No relevant results found.\"\n",
    "    else:\n",
    "        return f\"SerpAPI error: {response.status_code} {response.text}\"\n",
    "\n",
    "@observe()\n",
    "def parse_excel_csv(file_name):\n",
    "    \"\"\"\n",
    "    Reads an Excel or CSV file and extracts schema, sample data, and summary statistics according to what the question asks for.\n",
    "    \"\"\"\n",
    "    import os\n",
    "    file_path = os.path.join(\"files-for-agent\", file_name)\n",
    "    try:\n",
    "        if file_path.lower().endswith('.csv'):\n",
    "            df = pd.read_csv(file_path)\n",
    "        else:\n",
    "            df = pd.read_excel(file_path)\n",
    "        schema = df.columns.tolist()\n",
    "        dtypes = df.dtypes.astype(str).to_dict()\n",
    "        sample = df.head(3).to_dict(orient=\"records\")\n",
    "        stats = df.describe(include='all').to_dict()\n",
    "        return {\n",
    "            \"schema\": schema,\n",
    "            \"dtypes\": dtypes,\n",
    "            \"sample\": sample,\n",
    "            \"stats\": stats\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return f\"Error parsing file {file_path}: {str(e)}\"\n",
    "\n",
    "@observe()\n",
    "def analyze_image_with_vision(file_name, question, model_name, temperature):\n",
    "    \"\"\"\n",
    "    Uses a vision-capable model to analyze an image and answer a question about it.\n",
    "    \"\"\"\n",
    "    import openai\n",
    "    import os\n",
    "    file_path = os.path.join(\"files-for-agent\", file_name)\n",
    "    api_key = config.get(\"openai_api_key\")\n",
    "    client = openai.OpenAI(api_key=api_key)\n",
    "    # Detect MIME type\n",
    "    mime_type, _ = mimetypes.guess_type(file_path)\n",
    "    if mime_type is None:\n",
    "        mime_type = \"image/png\"  # fallback\n",
    "    with open(file_path, \"rb\") as img_file:\n",
    "        image_bytes = img_file.read()\n",
    "    base64_image = base64.b64encode(image_bytes).decode(\"utf-8\")\n",
    "    data_url = f\"data:{mime_type};base64,{base64_image}\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": [\n",
    "                {\"type\": \"text\", \"text\": question},\n",
    "                {\"type\": \"image_url\", \"image_url\": {\"url\": data_url}}\n",
    "            ]}\n",
    "        ],\n",
    "        temperature=temperature\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Whisper Audio Transcription Tool\n",
    "@observe()\n",
    "def transcribe_audio_with_whisper(file_name, model_size=\"base\"):\n",
    "    \"\"\"\n",
    "    Transcribes an audio file (e.g., .mp3) using OpenAI Whisper.\n",
    "    Returns the transcribed text.\n",
    "    \"\"\"\n",
    "    import whisper\n",
    "    file_path = os.path.join(\"files-for-agent\", file_name)\n",
    "    try:\n",
    "        model = whisper.load_model(model_size)\n",
    "        result = model.transcribe(file_path)\n",
    "        return result[\"text\"]\n",
    "    except Exception as e:\n",
    "        return f\"Error transcribing audio {file_path}: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Agent Planning Step (OpenAI v1.x+)\n",
    "\n",
    "import openai\n",
    "from langfuse.decorators import observe  # (if not already imported)\n",
    "\n",
    "# Set your OpenAI API key from config\n",
    "client = openai.OpenAI(api_key=config[\"openai_api_key\"])\n",
    "\n",
    "@observe(as_type=\"generation\")\n",
    "def get_agent_plan(question, model_name, temperature, file_name=None):\n",
    "    \"\"\"\n",
    "    Sends the question to the model and asks for a plan and tool list.\n",
    "    The prompt now describes the Excel/CSV tool.\n",
    "    \"\"\"\n",
    "    prompt = (\n",
    "        \"You are an AI agent. Here is a question you need to answer:\\n\"\n",
    "        f\"Question: {question}\\n\\n\"\n",
    "        \"You have access to the following tools:\\n\"\n",
    "        \"- Wikipedia Search: For factual and encyclopedic information.\\n\"\n",
    "        \"- SerpAPI Web Search: For general web search (Google, Bing, etc.).\\n\"\n",
    "        \"- Excel/CSV File Parser: For reading Excel or CSV files attached to the question.\\n\"\n",
    "        \"- Image Analysis (Vision): For analyzing images attached to the question. This tool sends the image and the question to a vision-capable model and returns the model's answer.\\n\\n\"\n",
    "        \"- Audio Transcription(Whisper): For transcribing audio files (eg: .mp3) to text that are attached to the question. This tool sends the audio file to a transcription model and returns the transcribed text from OpenAI Whisper.\\n\\n\"\n",
    "        \"If a file is attached and it is an image (e.g., .png, .jpg, .jpeg), use the Image Analysis tool to analyze the image and answer the question.\\n\"\n",
    "        \"Create a step-by-step plan to answer this question. For each step, specify which tool you would use and why. \"\n",
    "        \"Be explicit about your reasoning for tool selection.\"\n",
    "    )\n",
    "    if file_name:\n",
    "        prompt += f\"\\n\\nA file is attached: {file_name}\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=temperature\n",
    "    )\n",
    "    plan = response.choices[0].message.content\n",
    "    return plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Tool Execution Step\n",
    "\n",
    "from langfuse.decorators import observe\n",
    "import re\n",
    "\n",
    "def execute_tools(plan, question, file_name=None, temperature=temperature):\n",
    "    tool_outputs = {}\n",
    "    plan_lower = plan.lower()\n",
    "\n",
    "    def tool_in_plan(tool_name):\n",
    "        return tool_name in plan_lower\n",
    "    \n",
    "    # Audio file transcription\n",
    "    if file_name and file_name.lower().endswith(('.mp3', '.wav', '.m4a', '.flac', '.ogg')):\n",
    "        # Always transcribe audio if present\n",
    "        audio_result = transcribe_audio_with_whisper(file_name)\n",
    "        tool_outputs['audio_transcription'] = audio_result\n",
    "\n",
    "    # Image file analysis\n",
    "    if file_name and file_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "        if tool_in_plan(\"image analysis\") or tool_in_plan(\"vision\") or tool_in_plan(\"analyze image\"):\n",
    "            image_result = analyze_image_with_vision(file_name, question, model_name, temperature)\n",
    "            tool_outputs['image_analysis'] = image_result\n",
    "        else:\n",
    "            # Optionally, always analyze if image is present\n",
    "            image_result = analyze_image_with_vision(file_name, question, model_name, temperature)\n",
    "            tool_outputs['image_analysis'] = image_result\n",
    "\n",
    "    # Excel/CSV file parsing (existing)\n",
    "    if file_name and file_name.lower().endswith(('.xlsx', '.xls', '.csv')):\n",
    "        excel_result = parse_excel_csv(file_name)\n",
    "        tool_outputs['excel_csv'] = excel_result\n",
    "\n",
    "    # Wikipedia\n",
    "    if tool_in_plan(\"wikipedia\"):\n",
    "        wiki_result = wikipedia_search(question)\n",
    "        tool_outputs['wikipedia'] = wiki_result\n",
    "    else:\n",
    "        wiki_result = None\n",
    "\n",
    "    # SerpAPI Web Search\n",
    "    if tool_in_plan(\"serpapi web\") or tool_in_plan(\"web search\") or tool_in_plan(\"serpapi search\") or tool_in_plan(\"google search\"):\n",
    "        serp_result = serpapi_search(question)\n",
    "        tool_outputs['serpapi'] = serp_result\n",
    "\n",
    "    # Fallback: If Wikipedia was run and is insufficient, and SerpAPI web search wasn't already run, run it\n",
    "    fallback_needed = (\n",
    "        wiki_result is not None and (\n",
    "            \"no wikipedia page found\" in wiki_result.lower() or\n",
    "            \"disambiguation error\" in wiki_result.lower() or\n",
    "            \"error:\" in wiki_result.lower() or\n",
    "            len(wiki_result) < 50\n",
    "        ) and 'serpapi' not in tool_outputs\n",
    "    )\n",
    "    if fallback_needed:\n",
    "        serp_result = serpapi_search(question)\n",
    "        tool_outputs['serpapi'] = serp_result\n",
    "        print(\"Fallback: Used SerpAPI web search due to insufficient Wikipedia result.\")\n",
    "\n",
    "    print(\"Tools used for this question:\", list(tool_outputs.keys()))\n",
    "    return tool_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Synthesis Step (OpenAI v1.x+)\n",
    "\n",
    "from langfuse.decorators import observe  # (if not already imported)\n",
    "\n",
    "@observe(as_type=\"generation\")\n",
    "def synthesize_final_answer(task_id, question, tool_outputs, gaia_doc, model_name, temperature):\n",
    "    \"\"\"\n",
    "    Uses the model to synthesize a final answer in GAIA format.\n",
    "    The prompt is enhanced to force careful question analysis, step-by-step reasoning, and self-critique.\n",
    "    \"\"\"\n",
    "    prompt = (\n",
    "        f\"You are an AI agent participating in the GAIA benchmark.\\n\"\n",
    "        f\"Here is the official GAIA documentation for answer formatting:\\n\\n\"\n",
    "        f\"{gaia_doc}\\n\\n\"\n",
    "        f\"Here is the original question:\\n{question}\\n\\n\"\n",
    "        f\"Here are the outputs from the tools you used:\\n{tool_outputs}\\n\\n\"\n",
    "        \"Your task is to generate the most accurate and complete answer possible, strictly following the question's requirements and the GAIA format.\\n\\n\"\n",
    "        \"Follow these steps:\\n\"\n",
    "        \"1. Carefully re-read and break down the question. List all requirements, constraints, and any special instructions (e.g., only list ingredients, exclude measurements, alphabetize, etc.).\\n\"\n",
    "        \"2. Summarize the relevant information from the tool outputs that directly addresses the question.\\n\"\n",
    "        \"3. Step-by-step, reason through how to answer the question, making sure to address every requirement and constraint you listed in step 1.\\n\"\n",
    "        \"4. Critique your draft answer: Does it fully and precisely answer the question? Does it include anything extra or miss anything required? If so, revise it.\\n\"\n",
    "        \"5. Only after this, output the final answer in the required GAIA JSON format. Only output the JSON object, nothing else.\\n\"\n",
    "        \"Show your work for steps 1-4 as comments (using lines starting with #), then output the JSON answer.\"\n",
    "    )\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=temperature\n",
    "    )\n",
    "    final_answer_json = response.choices[0].message.content\n",
    "    return final_answer_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Main Agent Loop with Langfuse Traceability\n",
    "\n",
    "from langfuse.decorators import observe, langfuse_context\n",
    "\n",
    "@observe()\n",
    "def process_all_questions(questions, model_name, temperature, gaia_doc):\n",
    "    final_answers = []\n",
    "    for q in questions:\n",
    "        print(f\"Processing Task ID: {q['task_id']}\")\n",
    "        plan = get_agent_plan(q['question'], model_name, temperature, file_name=q.get(\"file_name\"))\n",
    "        tool_outputs = execute_tools(plan, q['question'], file_name=q.get(\"file_name\"), temperature=temperature)\n",
    "        # Use the correct task_id from the question\n",
    "        final_answer_json = synthesize_final_answer(\n",
    "            task_id=q['task_id'],\n",
    "            question=q['question'],\n",
    "            tool_outputs=tool_outputs,\n",
    "            gaia_doc=gaia_doc,\n",
    "            model_name=model_name,\n",
    "            temperature=temperature\n",
    "        )\n",
    "        final_answers.append(final_answer_json)\n",
    "        # Print trace URL for traceability\n",
    "        print(\"Langfuse Trace URL:\", langfuse_context.get_current_trace_url())\n",
    "    return final_answers\n",
    "\n",
    "# Load GAIA documentation from file\n",
    "with open(\"documentation/GIAI-documentation.md\", \"r\") as f:\n",
    "    gaia_doc = f.read()\n",
    "\n",
    "# Process all questions:\n",
    "final_answers = process_all_questions(questions, model_name, temperature, gaia_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9: Save Results (with cleaning and validation)\n",
    "\n",
    "import json\n",
    "import re\n",
    "\n",
    "def extract_json_from_response(answer_str):\n",
    "    \"\"\"\n",
    "    Extracts the first JSON object found in the string.\n",
    "    \"\"\"\n",
    "    # Remove code block markers if present\n",
    "    answer_str = answer_str.strip()\n",
    "    answer_str = re.sub(r\"^```[a-zA-Z]*\", \"\", answer_str)\n",
    "    answer_str = re.sub(r\"```$\", \"\", answer_str).strip()\n",
    "    # Find the first JSON object in the string\n",
    "    match = re.search(r\"\\{[\\s\\S]*\\}\", answer_str)\n",
    "    if match:\n",
    "        return match.group(0)\n",
    "    else:\n",
    "        raise ValueError(f\"No JSON object found in answer:\\n{answer_str}\")\n",
    "\n",
    "def clean_and_validate_answer(answer_str, required_fields=(\"task_id\", \"submitted_answer\"), correct_task_id=None):\n",
    "    try:\n",
    "        json_str = extract_json_from_response(answer_str)\n",
    "        answer_obj = json.loads(json_str)\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Invalid JSON: {e}\\nRaw output: {answer_str}\")\n",
    "    if correct_task_id is not None:\n",
    "        answer_obj[\"task_id\"] = correct_task_id\n",
    "    for field in required_fields:\n",
    "        if field not in answer_obj:\n",
    "            raise ValueError(f\"Missing required field '{field}' in answer: {answer_obj}\")\n",
    "    return answer_obj\n",
    "\n",
    "def save_final_answers(final_answers, questions, filename=\"all-json/final_answers.jsonl\"):\n",
    "    cleaned_answers = []\n",
    "    for i, answer in enumerate(final_answers):\n",
    "        correct_task_id = questions[i][\"task_id\"]\n",
    "        if isinstance(answer, str):\n",
    "            try:\n",
    "                answer_obj = clean_and_validate_answer(answer, correct_task_id=correct_task_id)\n",
    "            except Exception as e:\n",
    "                print(f\"Error in answer {i}: {e}\")\n",
    "                continue\n",
    "        else:\n",
    "            answer_obj = answer\n",
    "            answer_obj[\"task_id\"] = correct_task_id\n",
    "        cleaned_answers.append(answer_obj)\n",
    "    with open(filename, \"w\") as f:\n",
    "        for answer_obj in cleaned_answers:\n",
    "            f.write(json.dumps(answer_obj, ensure_ascii=False) + \"\\n\")\n",
    "    print(f\"Saved {len(cleaned_answers)} answers to {filename}\")\n",
    "    return cleaned_answers\n",
    "\n",
    "# Example usage:\n",
    "final_answers_cleaned = save_final_answers(final_answers, questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 10: Validate Answers (Check with GAIA API)\n",
    "\n",
    "import requests\n",
    "\n",
    "def validate_answers(final_answers, username, code_link, api_base_url, agent_code=None):\n",
    "    \"\"\"\n",
    "    Submits answers to the GAIA evaluation endpoint for validation.\n",
    "    Prints the score and which answers were correct.\n",
    "    \"\"\"\n",
    "    url = f\"{api_base_url}/submit\"\n",
    "    if agent_code is None:\n",
    "        # Try to read your notebook as code, or use code_link as fallback\n",
    "        try:\n",
    "            with open(\"agent.ipynb\", \"r\") as f:\n",
    "                agent_code = f.read()\n",
    "        except Exception:\n",
    "            agent_code = code_link  # fallback\n",
    "    payload = {\n",
    "        \"username\": username,\n",
    "        \"code_link\": code_link,\n",
    "        \"agent_code\": agent_code,\n",
    "        \"answers\": final_answers  # <-- Use the cleaned answers here!\n",
    "    }\n",
    "    response = requests.post(url, json=payload)\n",
    "    if response.status_code == 200:\n",
    "        result = response.json()\n",
    "        print(\"Submission successful!\")\n",
    "        print(f\"Score: {result.get('score', 'N/A')}%\")\n",
    "        if \"results\" in result:\n",
    "            print(\"\\nDetailed Results:\")\n",
    "            for r in result[\"results\"]:\n",
    "                status = \"✅\" if r.get(\"correct\") else \"❌\"\n",
    "                print(f\"{status} Task ID: {r['task_id']} | Your Answer: {r['submitted_answer']} | Correct: {r.get('correct_answer', 'N/A')}\")\n",
    "        else:\n",
    "            print(\"No detailed results returned.\")\n",
    "        return result\n",
    "    else:\n",
    "        print(\"Submission failed:\", response.status_code, response.text)\n",
    "        return None\n",
    "\n",
    "# Example usage:\n",
    "validation_result = validate_answers(final_answers_cleaned, username, code_link, api_base_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 11: (Optional) Save Validation Results\n",
    "\n",
    "def save_validation_results(validation_result, filename=\"all-json/validation_results.json\"):\n",
    "    if validation_result is not None:\n",
    "        with open(filename, \"w\") as f:\n",
    "            json.dump(validation_result, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"Validation results saved to {filename}\")\n",
    "\n",
    "# Example usage:\n",
    "save_validation_results(validation_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 12: Save Validation Results (with cleaning and validation)\n",
    "\n",
    "# --- Langfuse flush at the end of the notebook ---\n",
    "from langfuse.decorators import langfuse_context  # (if not already imported)\n",
    "langfuse_context.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_agents_course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
