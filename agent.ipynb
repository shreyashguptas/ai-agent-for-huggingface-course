{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install langfuse wikipedia openai google-search-results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config loaded. Sensitive keys available for use.\n",
      "Notebook variables set:\n",
      "  model_name: gpt-4.1-2025-04-14\n",
      "  temperature: 0.0\n",
      "  verbose: True\n",
      "  use_langfuse: True\n",
      "  username: Shreyashgupta5\n",
      "  code_link: https://huggingface.co/spaces/Shreyashgupta5/ai_agents_course\n",
      "  api_base_url: https://agents-course-unit4-scoring.hf.space\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load Configuration and Dependencies\n",
    "\n",
    "import json\n",
    "\n",
    "# Load sensitive config from config.json\n",
    "with open('config.json', 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# Set notebook variables (these should match what you set in your notebook)\n",
    "temperature = 0.0\n",
    "verbose = True\n",
    "use_langfuse = True\n",
    "model_name = \"gpt-4.1-2025-04-14\"\n",
    "username = \"Shreyashgupta5\"\n",
    "code_link = \"https://huggingface.co/spaces/Shreyashgupta5/ai_agents_course\"\n",
    "api_base_url = \"https://agents-course-unit4-scoring.hf.space\"\n",
    "\n",
    "# Print to verify\n",
    "print(\"Config loaded. Sensitive keys available for use.\")\n",
    "print(\"Notebook variables set:\")\n",
    "print(f\"  model_name: {model_name}\")\n",
    "print(f\"  temperature: {temperature}\")\n",
    "print(f\"  verbose: {verbose}\")\n",
    "print(f\"  use_langfuse: {use_langfuse}\")\n",
    "print(f\"  username: {username}\")\n",
    "print(f\"  code_link: {code_link}\")\n",
    "print(f\"  api_base_url: {api_base_url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Configure Langfuse Decorator-Based Client\n",
    "\n",
    "from langfuse.decorators import langfuse_context\n",
    "\n",
    "langfuse_context.configure(\n",
    "    secret_key=config[\"langfuse_secret\"],\n",
    "    public_key=config[\"langfuse_public_key\"],\n",
    "    host=config[\"host\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task ID: 8e867cd7-cff9-4e6c-867a-ff5ddc2550be\n",
      "Question: How many studio albums were published by Mercedes Sosa between 2000 and 2009 (included)? You can use the latest 2022 version of english wikipedia.\n",
      "----------------------------------------\n",
      "Task ID: a1e91b78-d3d8-4675-bb8d-62741b4b68a6\n",
      "Question: In the video https://www.youtube.com/watch?v=L1vXCYZAYYM, what is the highest number of bird species to be on camera simultaneously?\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Load Questions\n",
    "\n",
    "import json\n",
    "\n",
    "# Set how many questions you want to process\n",
    "NUM_QUESTIONS_TO_RUN = 20  # <--- Change this number as needed\n",
    "\n",
    "# Load all questions from all_questions.json\n",
    "with open('all_questions.json', 'r') as f:\n",
    "    all_questions = json.load(f)\n",
    "\n",
    "# Only process up to NUM_QUESTIONS_TO_RUN questions\n",
    "questions = all_questions[:NUM_QUESTIONS_TO_RUN]\n",
    "\n",
    "# Print out each question's task_id and question text for verification\n",
    "for q in questions:\n",
    "    print(f\"Task ID: {q['task_id']}\")\n",
    "    print(f\"Question: {q['question']}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Define Tools\n",
    "\n",
    "import wikipedia\n",
    "from langfuse.decorators import observe\n",
    "import os\n",
    "import requests\n",
    "\n",
    "# Wikipedia Search Tool\n",
    "@observe()\n",
    "def wikipedia_search(query, sentences=2):\n",
    "    try:\n",
    "        summary = wikipedia.summary(query, sentences=sentences, auto_suggest=True, redirect=True)\n",
    "        return summary\n",
    "    except wikipedia.DisambiguationError as e:\n",
    "        return f\"Disambiguation error. Options: {e.options[:5]}\"\n",
    "    except wikipedia.PageError:\n",
    "        return \"No Wikipedia page found for the query.\"\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "# SerpAPI Web Search Tool\n",
    "@observe()\n",
    "def serpapi_search(query):\n",
    "    api_key = config.get(\"SERPAPI_API_KEY\") or os.environ.get(\"SERPAPI_API_KEY\")\n",
    "    if not api_key:\n",
    "        return \"No SerpAPI key provided.\"\n",
    "    params = {\n",
    "        \"q\": query,\n",
    "        \"api_key\": api_key,\n",
    "        \"engine\": \"google\",\n",
    "        \"num\": 3\n",
    "    }\n",
    "    response = requests.get(\"https://serpapi.com/search\", params=params)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        if \"answer_box\" in data and \"answer\" in data[\"answer_box\"]:\n",
    "            return data[\"answer_box\"][\"answer\"]\n",
    "        elif \"organic_results\" in data and len(data[\"organic_results\"]) > 0:\n",
    "            return data[\"organic_results\"][0].get(\"snippet\", \"No snippet found.\")\n",
    "        else:\n",
    "            return \"No relevant results found.\"\n",
    "    else:\n",
    "        return f\"SerpAPI error: {response.status_code} {response.text}\"\n",
    "\n",
    "# SerpAPI Image Search Tool\n",
    "@observe()\n",
    "def serpapi_image_search(query):\n",
    "    \"\"\"\n",
    "    Uses SerpAPI's Google Images API to search for images related to the query.\n",
    "    Returns the first image result's URL and title.\n",
    "    Docs: https://serpapi.com/images-results\n",
    "    \"\"\"\n",
    "    api_key = config.get(\"SERPAPI_API_KEY\") or os.environ.get(\"SERPAPI_API_KEY\")\n",
    "    if not api_key:\n",
    "        return \"No SerpAPI key provided.\"\n",
    "    params = {\n",
    "        \"q\": query,\n",
    "        \"api_key\": api_key,\n",
    "        \"engine\": \"google_images\"\n",
    "    }\n",
    "    response = requests.get(\"https://serpapi.com/search\", params=params)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        images = data.get(\"images_results\", [])\n",
    "        if images:\n",
    "            first = images[0]\n",
    "            return {\n",
    "                \"title\": first.get(\"title\"),\n",
    "                \"image_url\": first.get(\"original\"),\n",
    "                \"thumbnail\": first.get(\"thumbnail\"),\n",
    "                \"source\": first.get(\"source\")\n",
    "            }\n",
    "        else:\n",
    "            return \"No images found.\"\n",
    "    else:\n",
    "        return f\"SerpAPI error: {response.status_code} {response.text}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Agent Planning Step (OpenAI v1.x+)\n",
    "\n",
    "import openai\n",
    "from langfuse.decorators import observe  # (if not already imported)\n",
    "\n",
    "# Set your OpenAI API key from config\n",
    "client = openai.OpenAI(api_key=config[\"openai_api_key\"])\n",
    "\n",
    "@observe(as_type=\"generation\")\n",
    "def get_agent_plan(question, model_name, temperature=0.0):\n",
    "    \"\"\"\n",
    "    Sends the question to the model and asks for a plan and tool list.\n",
    "    The prompt now describes all available tools, including image search.\n",
    "    \"\"\"\n",
    "    prompt = (\n",
    "        \"You are an AI agent. Here is a question you need to answer:\\n\"\n",
    "        f\"Question: {question}\\n\\n\"\n",
    "        \"You have access to the following tools:\\n\"\n",
    "        \"- Wikipedia Search: For factual and encyclopedic information.\\n\"\n",
    "        \"- SerpAPI Web Search: For general web search (Google, Bing, etc.).\\n\"\n",
    "        \"- SerpAPI Image Search: For finding images or when the question is about pictures/photos.\\n\"\n",
    "        \"Create a step-by-step plan to answer this question. For each step, specify which tool you would use and why. \"\n",
    "        \"If the question is about images, use the image search tool. \"\n",
    "        \"If Wikipedia is insufficient, fall back to SerpAPI Web Search. \"\n",
    "        \"Be explicit about your reasoning for tool selection.\"\n",
    "    )\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=temperature\n",
    "    )\n",
    "    plan = response.choices[0].message.content\n",
    "    return plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Tool Execution Step\n",
    "\n",
    "from langfuse.decorators import observe\n",
    "import re\n",
    "\n",
    "@observe()\n",
    "def execute_tools(plan, question):\n",
    "    \"\"\"\n",
    "    Executes tools based on the tools mentioned in the plan.\n",
    "    - Runs only the tools mentioned in the plan.\n",
    "    - Still falls back to SerpAPI web search if Wikipedia is insufficient and not already in the plan.\n",
    "    \"\"\"\n",
    "    tool_outputs = {}\n",
    "\n",
    "    # Normalize plan to lower case for matching\n",
    "    plan_lower = plan.lower()\n",
    "\n",
    "    # Helper: check if a tool is mentioned in the plan\n",
    "    def tool_in_plan(tool_name):\n",
    "        return tool_name in plan_lower\n",
    "\n",
    "    # Run tools as per plan\n",
    "    if tool_in_plan(\"wikipedia\"):\n",
    "        wiki_result = wikipedia_search(question)\n",
    "        tool_outputs['wikipedia'] = wiki_result\n",
    "    else:\n",
    "        wiki_result = None\n",
    "\n",
    "    if tool_in_plan(\"image\"):\n",
    "        image_result = serpapi_image_search(question)\n",
    "        tool_outputs['serpapi_image'] = image_result\n",
    "\n",
    "    if tool_in_plan(\"serpapi web\") or tool_in_plan(\"web search\") or tool_in_plan(\"serpapi search\") or tool_in_plan(\"google search\"):\n",
    "        serp_result = serpapi_search(question)\n",
    "        tool_outputs['serpapi'] = serp_result\n",
    "\n",
    "    # Fallback: If Wikipedia was run and is insufficient, and SerpAPI web search wasn't already run, run it\n",
    "    fallback_needed = (\n",
    "        wiki_result is not None and (\n",
    "            \"no wikipedia page found\" in wiki_result.lower() or\n",
    "            \"disambiguation error\" in wiki_result.lower() or\n",
    "            \"error:\" in wiki_result.lower() or\n",
    "            len(wiki_result) < 50  # You can adjust this threshold\n",
    "        ) and 'serpapi' not in tool_outputs\n",
    "    )\n",
    "    if fallback_needed:\n",
    "        serp_result = serpapi_search(question)\n",
    "        tool_outputs['serpapi'] = serp_result\n",
    "        print(\"Fallback: Used SerpAPI web search due to insufficient Wikipedia result.\")\n",
    "\n",
    "    # Print which tools were used\n",
    "    print(\"Tools used for this question:\", list(tool_outputs.keys()))\n",
    "\n",
    "    return tool_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Synthesis Step (OpenAI v1.x+)\n",
    "\n",
    "from langfuse.decorators import observe  # (if not already imported)\n",
    "\n",
    "@observe(as_type=\"generation\")\n",
    "def synthesize_final_answer(task_id, question, tool_outputs, gaia_doc, model_name, temperature=0.0):\n",
    "    \"\"\"\n",
    "    Uses the model to synthesize a final answer in GAIA format.\n",
    "    \"\"\"\n",
    "    prompt = (\n",
    "        f\"You are an AI agent participating in the GAIA benchmark. \"\n",
    "        f\"Here is the official GAIA documentation for answer formatting:\\n\\n\"\n",
    "        f\"{gaia_doc}\\n\\n\"\n",
    "        f\"Here is the original question:\\n{question}\\n\\n\"\n",
    "        f\"Here are the outputs from the tools you used:\\n{tool_outputs}\\n\\n\"\n",
    "        \"If the Wikipedia output is sufficient and correct, use it. \"\n",
    "        \"If not, use the SerpAPI output. If neither is sufficient, say so. \"\n",
    "        \"Using the information above, generate the final answer in the required GAIA JSON format. \"\n",
    "        \"Only output the JSON object, nothing else.\"\n",
    "    )\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=temperature\n",
    "    )\n",
    "    final_answer_json = response.choices[0].message.content\n",
    "    return final_answer_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Task ID: 8e867cd7-cff9-4e6c-867a-ff5ddc2550be\n",
      "Tools used for this question: ['wikipedia', 'serpapi']\n",
      "Langfuse Trace URL: https://us.cloud.langfuse.com/project/cmabtwja701n8ad06grpw13lr/traces/74d0887a-3098-43f6-b52e-be8aa7abc1d6\n",
      "Processing Task ID: a1e91b78-d3d8-4675-bb8d-62741b4b68a6\n",
      "Tools used for this question: ['serpapi_image', 'serpapi_youtube', 'serpapi']\n",
      "Langfuse Trace URL: https://us.cloud.langfuse.com/project/cmabtwja701n8ad06grpw13lr/traces/74d0887a-3098-43f6-b52e-be8aa7abc1d6\n"
     ]
    }
   ],
   "source": [
    "# Step 8: Main Agent Loop with Langfuse Traceability\n",
    "\n",
    "from langfuse.decorators import observe, langfuse_context  # (if not already imported)\n",
    "\n",
    "@observe()\n",
    "def process_all_questions(questions, model_name, temperature, gaia_doc):\n",
    "    final_answers = []\n",
    "    for q in questions:\n",
    "        print(f\"Processing Task ID: {q['task_id']}\")\n",
    "        plan = get_agent_plan(q['question'], model_name, temperature)\n",
    "        tool_outputs = execute_tools(plan, q['question'])\n",
    "        # Use the correct task_id from the question\n",
    "        final_answer_json = synthesize_final_answer(\n",
    "            task_id=q['task_id'],\n",
    "            question=q['question'],\n",
    "            tool_outputs=tool_outputs,\n",
    "            gaia_doc=gaia_doc,\n",
    "            model_name=model_name,\n",
    "            temperature=temperature\n",
    "        )\n",
    "        final_answers.append(final_answer_json)\n",
    "        # Print trace URL for traceability\n",
    "        print(\"Langfuse Trace URL:\", langfuse_context.get_current_trace_url())\n",
    "    return final_answers\n",
    "\n",
    "# Load GAIA documentation from file\n",
    "with open(\"documentation/GIAI-documentation.md\", \"r\") as f:\n",
    "    gaia_doc = f.read()\n",
    "\n",
    "# Process all questions:\n",
    "final_answers = process_all_questions(questions, model_name, temperature, gaia_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 2 answers to final_answers.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Step 9: Save Results (with cleaning and validation)\n",
    "\n",
    "import json\n",
    "import re\n",
    "\n",
    "def clean_and_validate_answer(answer_str, required_fields=(\"task_id\", \"submitted_answer\"), correct_task_id=None):\n",
    "    answer_str = answer_str.strip()\n",
    "    answer_str = re.sub(r\"^```[a-zA-Z]*\", \"\", answer_str)\n",
    "    answer_str = re.sub(r\"```$\", \"\", answer_str).strip()\n",
    "    try:\n",
    "        answer_obj = json.loads(answer_str)\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Invalid JSON: {e}\\nRaw output: {answer_str}\")\n",
    "    if correct_task_id is not None:\n",
    "        answer_obj[\"task_id\"] = correct_task_id\n",
    "    for field in required_fields:\n",
    "        if field not in answer_obj:\n",
    "            raise ValueError(f\"Missing required field '{field}' in answer: {answer_obj}\")\n",
    "    return answer_obj\n",
    "\n",
    "def save_final_answers(final_answers, questions, filename=\"final_answers.jsonl\"):\n",
    "    cleaned_answers = []\n",
    "    for i, answer in enumerate(final_answers):\n",
    "        correct_task_id = questions[i][\"task_id\"]\n",
    "        if isinstance(answer, str):\n",
    "            try:\n",
    "                answer_obj = clean_and_validate_answer(answer, correct_task_id=correct_task_id)\n",
    "            except Exception as e:\n",
    "                print(f\"Error in answer {i}: {e}\")\n",
    "                continue\n",
    "        else:\n",
    "            answer_obj = answer\n",
    "            answer_obj[\"task_id\"] = correct_task_id\n",
    "        cleaned_answers.append(answer_obj)\n",
    "    with open(filename, \"w\") as f:\n",
    "        for answer_obj in cleaned_answers:\n",
    "            f.write(json.dumps(answer_obj, ensure_ascii=False) + \"\\n\")\n",
    "    print(f\"Saved {len(cleaned_answers)} answers to {filename}\")\n",
    "    return cleaned_answers\n",
    "\n",
    "# Example usage:\n",
    "final_answers_cleaned = save_final_answers(final_answers, questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission successful!\n",
      "Score: 0.0%\n",
      "No detailed results returned.\n"
     ]
    }
   ],
   "source": [
    "# Step 10: Validate Answers (Check with GAIA API)\n",
    "\n",
    "import requests\n",
    "\n",
    "def validate_answers(final_answers, username, code_link, api_base_url, agent_code=None):\n",
    "    \"\"\"\n",
    "    Submits answers to the GAIA evaluation endpoint for validation.\n",
    "    Prints the score and which answers were correct.\n",
    "    \"\"\"\n",
    "    url = f\"{api_base_url}/submit\"\n",
    "    if agent_code is None:\n",
    "        # Try to read your notebook as code, or use code_link as fallback\n",
    "        try:\n",
    "            with open(\"agent.ipynb\", \"r\") as f:\n",
    "                agent_code = f.read()\n",
    "        except Exception:\n",
    "            agent_code = code_link  # fallback\n",
    "    payload = {\n",
    "        \"username\": username,\n",
    "        \"code_link\": code_link,\n",
    "        \"agent_code\": agent_code,\n",
    "        \"answers\": final_answers  # <-- Use the cleaned answers here!\n",
    "    }\n",
    "    response = requests.post(url, json=payload)\n",
    "    if response.status_code == 200:\n",
    "        result = response.json()\n",
    "        print(\"Submission successful!\")\n",
    "        print(f\"Score: {result.get('score', 'N/A')}%\")\n",
    "        if \"results\" in result:\n",
    "            print(\"\\nDetailed Results:\")\n",
    "            for r in result[\"results\"]:\n",
    "                status = \"✅\" if r.get(\"correct\") else \"❌\"\n",
    "                print(f\"{status} Task ID: {r['task_id']} | Your Answer: {r['submitted_answer']} | Correct: {r.get('correct_answer', 'N/A')}\")\n",
    "        else:\n",
    "            print(\"No detailed results returned.\")\n",
    "        return result\n",
    "    else:\n",
    "        print(\"Submission failed:\", response.status_code, response.text)\n",
    "        return None\n",
    "\n",
    "# Example usage:\n",
    "validation_result = validate_answers(final_answers_cleaned, username, code_link, api_base_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation results saved to validation_results.json\n"
     ]
    }
   ],
   "source": [
    "# Step 11: (Optional) Save Validation Results\n",
    "\n",
    "def save_validation_results(validation_result, filename=\"validation_results.json\"):\n",
    "    if validation_result is not None:\n",
    "        with open(filename, \"w\") as f:\n",
    "            json.dump(validation_result, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"Validation results saved to {filename}\")\n",
    "\n",
    "# Example usage:\n",
    "save_validation_results(validation_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 12: Save Validation Results (with cleaning and validation)\n",
    "\n",
    "# --- Langfuse flush at the end of the notebook ---\n",
    "from langfuse.decorators import langfuse_context  # (if not already imported)\n",
    "langfuse_context.flush()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_agents_course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
